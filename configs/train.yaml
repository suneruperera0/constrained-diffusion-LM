# ConstrainedDiffusionLM Training Configuration

model:
  dim: 512
  num_layers: 6
  num_heads: 8
  dim_feedforward: 2048
  dropout: 0.1
  max_seq_len: 256
  vocab_size: null

diffusion:
  timesteps: 1000
  schedule: "cosine"
  noise_type: "mask"

training:
  epochs: 100
  batch_size: 32
  learning_rate: 1.0e-4
  weight_decay: 0.01
  warmup_steps: 1000
  grad_clip: 1.0
  optimizer: "adamw"
  betas: [0.9, 0.999]
  eps: 1.0e-8

data:
  train_path: "data/processed/train.jsonl"
  val_path: "data/processed/val.jsonl"
  tokenizer: "bert-base-uncased"
  max_seq_len: 256
  num_workers: 4

checkpoint:
  output_dir: "checkpoints"
  save_every: 5
  keep_last: 3

logging:
  log_every: 100
  eval_every: 1
  wandb: false
  wandb_project: "constrained-diffusion-lm"

device: "auto"
seed: 42
